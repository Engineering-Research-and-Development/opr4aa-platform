<h1>On-The-Edge Processing Component for Acquisition and Actuation (OPR4AA)</h1>
OPR4AA stands for On-the-edge PRocessing for Acquisition and Actuation: it is a platform based on open source FIWARE/Apache components aiming to cover the entire industrial data value chain. Built on top of the <a href="https://github.com/Engineering-Research-and-Development/dida">DIDA</a> (Digital Industries Data Analytics) platform, it can be deployed for edge processing, collecting data, process them and communicate results with other external cloud modules

The component allows you to bring input data call some external Rest API, process them using AI algorithms (e.g. Python Tensorflow + Keras) persist them in the internal persistence layer or export them to external modules through Rest API.

The OPR4AA platform is composed by:
- <b>FIWARE Draco (deprecated)</b>: based on Apache NiFi. NiFi is a data-flow system based on the flow-based concept programming designed to automate the flow of data in systems and support direct and scalable graphics.
- <b>Apache Airflow</b>:  an open-source platform that allows users to programmatically author, schedule, and monitor workflows. It is designed to be highly scalable, extensible, and modular, making it ideal for creating complex data processing pipelines.
- <b>Apache Hadoop Distributed File System (HDFS)</b>: designed to reliably store very large files across machines in a large cluster
- <b>Apache Spark</b>: is an open-source parallel processing framework for running largescale data analytics applications across clustered computers. It can handle both batch and real-time analytics and data processing workloads.
- <b>Apache Livy</b>:  is a service allowing easy interaction with a Spark cluster over a REST interface. Through it, can be easily submitted Spark jobs or snippets of Spark code, synchronous or asynchronous result retrieval, as well as Spark Context management, everything via a simple REST interface or an RPC client library. Apache Livy also simplifies the interaction between Spark and application servers.  

<br>

<h2>Requirements</h2>
<ul>
    <li>Docker Engine</li>
    <li>Minimum 8GB RAM</li>
    <li>Docker Compose >= 1.29</li>
</ul>

<br>

<h2>How to run</h2>
<h3>Build & Run containers:</h3>
<b>Before starting, chose wether to run AirFlow or NiFi as ingestion module. Comment/Uncomment docker-compose.yml accordingly.</b>

<code>docker network create -d bridge network-bridge</code>

<code>docker-compose up --build -d</code>

<code>docker-compose -f airflow.yml up --build  -d</code>

<br>

<h3>Access the UIs</h3>

1. Airflow at (https://localhost:8087/)
1. NiFi at (https://localhost:8443/nifi)
2. HDFS at (http://localhost:9870/explorer.html)
3. Spark Master at (http://localhost:8080/)
4. Spark Worker at (http://localhost:8081/)
5. Livy UI at (http://localhost:8998/)

<br>


<h2>Configuration</h2>

<h3>Using Airflow</h3>
In Airflow you can configure your own data flow (see Airflow doc <a  href="https://airflow.apache.org/docs">here</a>) in a more lightweight way.
The solution provides processing on Spark. 

<h4>Airflow credentials</h4>
<table>
<tr>
<th>User</th>
<th>Password</th>
</tr>
<tr>
<td>airflow</td>
<td>airflow</td>
</tr>
</table>

<h4>How to run a DAG programmatically</h4>
```bash
curl --location --request POST 'http://localhost:8087/api/v1/dags/test-pipeline/dagRuns' \
    --header 'Authorization: Basic YWlyZmxvdzphaXJmbG93' \
    --header 'Content-Type: application/json' \
    --data-raw '{
        "conf": {
            "host": "api.host.cloud",
            "username": "*****",
            "password": "*****",
            "source_entity": {
                "entity_id": "OPR4AA-Execution-Test",
                "entity_type": "Entity-Type-Test",
                "attribute":"image"
            },
            "sink_entity": {
                "entity_id": "OPR4AA-Execution-Test",
                "entity_type": "Entity-Type-Test",
                "attribute":"evaluation"
            }
        }
    }'
```
Basic Authorization header is generated by base64 encoding of string "airflow:airflow"

A [Postman collection](OPR4AA-V2.postman_collection.json) for DAG Run is provided.


<br>

<h3>Using Draco</h3>
In Draco you can configure your own data flow (see NiFi doc <a  href="https://nifi.apache.org/docs/nifi-docs/html/getting-started.html">here</a>).
The solution provides processing on Draco or Spark. Algorithms & data ingestion can be done by calling the provided APIs.
Inside the template pre-loaded on Draco you can activate the flows you prefer to use and can configure each NiFi processor following the notes on the UI.

<h4>Draco/NiFi credentials</h4>
<table>
<tr>
<th>User</th>
<th>Password</th>
</tr>
<tr>
<td>admin</td>
<td>ctsBtRBKHRAx69EqUghvvgEvjnaLjFEB</td>
</tr>
</table>

Draco will start with pre-uploaded template:

- [template.xml](orchestration/templates/template.xml)


<br>

<h4>API (available only using Draco)</h4>

<table role="table">
    <thead>
        <tr align="center">
            <th>HTTP Method</th>
            <th>Port</th>
            <th>Service</th>
            <th>Description</th>
        </tr>
    </thead>
    <tbody>
      <tr>
          <td>POST</td>
          <td>8085</td>
          <td>/ingest-algorithm</td>
          <td>Algorithm data ingestion route. Accept a .zip file that contains algorithm files.</td>
      </tr>
        <tr>
          <td>POST</td>
          <td>8086</td>
          <td>/ingest-data</td>
          <td>Input data ingestion route. Accept a file.</td>
      </tr>
  </tbody>
</table>

A [Postman collection](OPR4AA.postman_collection.json) for algorithms & data ingestion is provided.

<br>

<h2>Test AI classifier algorithm</h2> 
The solution is provided with an example you can run to test the solution.
An AI Python image classifier is provided, including a pre-trained ready-to-use neural network model based on Tensorflow and Keras (see <a  href="https://www.tensorflow.org/tutorials/keras/classification">here</a>).
